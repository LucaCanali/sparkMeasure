{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter/Colab Notebook to Showcase sparkMeasure APIs for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Run on Google Colab Research: <img src=\"https://raw.githubusercontent.com/googlecolab/open_in_colab/master/images/icon128.png\">](https://colab.research.google.com/github/LucaCanali/sparkMeasure/blob/master/examples/SparkMeasure_Jupyter_Colab_Example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkMeasure is a tool for performance troubleshooting of Apache Spark workloads**  \n",
    "It simplifies the collection and analysis of Spark performance metrics. It is also intended as a working example of how to use Spark listeners for collecting and processing Spark executors task metrics data.\n",
    "\n",
    "**References:**\n",
    "- [https://github.com/LucaCanali/sparkMeasure](https://github.com/LucaCanali/sparkMeasure)  \n",
    "- sparkmeasure Python docs: [docs/Python_shell_and_Jupyter](https://github.com/LucaCanali/sparkMeasure/blob/master/docs/Python_shell_and_Jupyter.md)  \n",
    "\n",
    "**Architecture:**\n",
    "![sparkMeasure architecture diagram](https://github.com/LucaCanali/sparkMeasure/raw/master/docs/sparkMeasure_architecture_diagram.png)\n",
    "\n",
    "Author and contact: Luca.Canali@cern.ch  \n",
    "Last updated: March 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# Install PySpark/Spark \n",
    "\n",
    "!pip install pyspark\n",
    "\n",
    "# Install the Python wrapper API for spark-measure\n",
    "\n",
    "!pip install sparkmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start the Spark Session\n",
    "# This example uses Spark in local mode for simplicity.\n",
    "# You can modify master to use  YARN or K8S if available \n",
    "# This example uses sparkMeasure 0.24 for scala 2.12, taken from maven central\n",
    "\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .master(\"local[*]\") \\\n",
    " .appName(\"Test sparkmeasure instrumentation of Python/PySpark code\") \\\n",
    " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.12:0.25\")  \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|    Greeting|\n",
      "+---+------------+\n",
      "|  1|Hello world!|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test that Spark is working OK\n",
    "spark.sql(\"select 1 as id, 'Hello world!' as Greeting\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sparkMeasure\n",
    "# Load the Python API for sparkmeasure package\n",
    "# and attach the sparkMeasure Listener for stagemetrics to the active Spark session\n",
    "\n",
    "from sparkmeasure import StageMetrics\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 17\n",
      "elapsedTime => 718 (0.7 s)\n",
      "stageDuration => 473 (0.5 s)\n",
      "executorRunTime => 2518 (3 s)\n",
      "executorCpuTime => 2200 (2 s)\n",
      "executorDeserializeTime => 239 (0.2 s)\n",
      "executorDeserializeCpuTime => 102 (0.1 s)\n",
      "resultSerializationTime => 21 (21 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 34 (34 ms)\n",
      "resultSize => 16306 (15.9 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 1 duration => 62 (62 ms)\n",
      "Stage 2 duration => 360 (0.4 s)\n",
      "Stage 4 duration => 51 (51 ms)\n"
     ]
    }
   ],
   "source": [
    "# The easiest way to start using sparkMesure is with the \"runandmeasure\" method\n",
    "# This will execute your Spark action, return the results, and collect and aggregate execution metrics\n",
    "\n",
    "stagemetrics.runandmeasure(globals(), \"\"\"\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional stage-level executor metrics (memory usage info):\n",
      "\n",
      "Stage 1 JVMHeapMemory maxVal bytes => 204990016 (195.5 MB)\n",
      "Stage 1 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 2 JVMHeapMemory maxVal bytes => 204990016 (195.5 MB)\n",
      "Stage 2 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 4 JVMHeapMemory maxVal bytes => 204990016 (195.5 MB)\n",
      "Stage 4 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n"
     ]
    }
   ],
   "source": [
    "# Additionally, sparkMeasure collects executor metrics\n",
    "# This is how you can print the memory usage report\n",
    "# Note, If you receive the error message java.util.NoSuchElementException: key not found, \n",
    "# retry running the report after waiting for a few seconds.\n",
    "\n",
    "stagemetrics.print_memory_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 17\n",
      "elapsedTime => 444 (0.4 s)\n",
      "stageDuration => 361 (0.4 s)\n",
      "executorRunTime => 2156 (2 s)\n",
      "executorCpuTime => 1708 (2 s)\n",
      "executorDeserializeTime => 87 (87 ms)\n",
      "executorDeserializeCpuTime => 42 (42 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 8 (8 ms)\n",
      "resultSize => 16048 (15.7 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 5 duration => 31 (31 ms)\n",
      "Stage 6 duration => 315 (0.3 s)\n",
      "Stage 8 duration => 15 (15 ms)\n"
     ]
    }
   ],
   "source": [
    "# An equivalent API for collecting execution metrics is to explicitly wrap your Spark workload\n",
    "# into stagemetrics instrumentation, as in this example\n",
    "\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()\n",
    "\n",
    "stagemetrics.end()\n",
    "\n",
    "# Print a summary report\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for Jupyter notebooks\n",
    "# Define cell and line magic to wrap sparkmeasure instrumentation\n",
    "# See example in the next cell\n",
    "\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
    "\n",
    "@register_line_cell_magic\n",
    "def sparkmeasure(line, cell=None):\n",
    "    \"run and measure spark workload. Use: %sparkmeasure or %%sparkmeasure\"\n",
    "    val = cell if cell is not None else line\n",
    "    stagemetrics.begin()\n",
    "    eval(val)\n",
    "    stagemetrics.end()\n",
    "    stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 17\n",
      "elapsedTime => 361 (0.4 s)\n",
      "stageDuration => 297 (0.3 s)\n",
      "executorRunTime => 1819 (2 s)\n",
      "executorCpuTime => 1738 (2 s)\n",
      "executorDeserializeTime => 51 (51 ms)\n",
      "executorDeserializeCpuTime => 41 (41 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 8 (8 ms)\n",
      "resultSize => 16048 (15.7 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 9 duration => 27 (27 ms)\n",
      "Stage 10 duration => 255 (0.3 s)\n",
      "Stage 12 duration => 15 (15 ms)\n"
     ]
    }
   ],
   "source": [
    "%%sparkmeasure\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of using Task Metrics\n",
    "Collecting Spark task metrics at the granularity of each task completion has additional overhead\n",
    "compared to collecting at the stage-level.\n",
    "This option should only be used if you need data with this finer granularity, for example because you want\n",
    "to study skew effects, otherwise consider using stagemetrics aggregation as the preferred choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark task metrics:\n",
      "numTasks => 17\n",
      "successful tasks => 17\n",
      "speculative tasks => 0\n",
      "taskDuration => 2102 (2 s)\n",
      "schedulerDelayTime => 84 (84 ms)\n",
      "executorRunTime => 1955 (2 s)\n",
      "executorCpuTime => 1715 (2 s)\n",
      "executorDeserializeTime => 63 (63 ms)\n",
      "executorDeserializeCpuTime => 33 (33 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 1 (1 ms)\n",
      "gettingResultTime => 0 (0 ms)\n",
      "resultSize => 2667 (2.6 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n"
     ]
    }
   ],
   "source": [
    "from sparkmeasure import TaskMetrics\n",
    "taskmetrics = TaskMetrics(spark)\n",
    "\n",
    "taskmetrics.begin()\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()\n",
    "taskmetrics.end()\n",
    "\n",
    "taskmetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark task metrics:\n",
      "numTasks => 17\n",
      "successful tasks => 17\n",
      "speculative tasks => 0\n",
      "taskDuration => 2192 (2 s)\n",
      "schedulerDelayTime => 81 (81 ms)\n",
      "executorRunTime => 2045 (2 s)\n",
      "executorCpuTime => 1780 (2 s)\n",
      "executorDeserializeTime => 66 (66 ms)\n",
      "executorDeserializeCpuTime => 30 (30 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "gettingResultTime => 0 (0 ms)\n",
      "resultSize => 2667 (2.6 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n"
     ]
    }
   ],
   "source": [
    "taskmetrics.runandmeasure(globals(), \"\"\"\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
